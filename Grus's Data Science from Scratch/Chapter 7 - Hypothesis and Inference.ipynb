{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7 - Hypothesis and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What will we do with all this statistics and probability theory? The science part of data\n",
    "science frequently involves forming and testing hypotheses about our data and the\n",
    "processes that generate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, as data scientists, we’ll want to test whether a certain hypothesis is likely to be\n",
    "true. For our purposes, hypotheses are assertions like “this coin is fair” or “data scien‐\n",
    "tists prefer Python to R” or “people are more likely to navigate away from the page\n",
    "without ever reading the content if we pop up an irritating interstitial advertisement\n",
    "with a tiny, hard-to-find close button” that can be translated into statistics about data.\n",
    "Under various assumptions, those statistics can be thought of as observations of random variables from known distributions, which allows us to make statements about\n",
    "how likely those assumptions are to hold.\n",
    "\n",
    "In the classical setup, we have a null hypothesis H0 that represents some default position, and some alternative hypothesis H1 that we’d like to compare it with. We use statistics to decide whether we can reject H0 as false or not. This will probably make\n",
    "more sense with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Flipping a Coin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we have a coin and we want to test whether it’s fair. We’ll make the assumption that the coin has some probability p of landing heads, and so our null hypothesis $H_0$: p = 0.5 (the coin is fair). We’ll test this against the alternative hypothesis $H_1$: p ≠ 0.5 (the coin isn't fair).\n",
    "\n",
    "In particular, our test will involve flipping the coin some number n times and the random variable X is the number of heads in n-times coin flipping. Each coin flip is a Bernoulli trial, which means that X is a Binomial(n,p) random variable, which (as we saw in Chapter 6) we can approximate\n",
    "using the normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_approx_to_binomial(n,p):\n",
    "    \"\"\"Finds mu and sigma corresponding to a Binomial(n,p).\"\"\"\n",
    "    mu = n*p\n",
    "    sigma = math.sqrt(n*p*(1-p))\n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever a random variable follows a normal distribution, we can use normal_cdf\n",
    "to figure out the probability that its realized value lies within (or outside) a particular\n",
    "interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_cdf(x, mu=0, sigma=1):\n",
    "    return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the normal cdf _is_ the probability the variable is below a threshold\n",
    "normal_prob_below = normal_cdf\n",
    "\n",
    "# it's above the threshold if it's not below the threshold\n",
    "def normal_prob_above(lo, mu=0, sigma=1):\n",
    "    return 1-normal_cdf(lo, mu, sigma)\n",
    "\n",
    "# it's between if it's less than hi, but not less than lo\n",
    "def normal_prob_between(lo, hi, mu=0, sigma=1):\n",
    "    return normal_cdf(hi,mu,sigma) - normal_cdf(lo,mu,sigma)\n",
    "\n",
    "# it's outside if it's not between\n",
    "def normal_prob_outside(lo, hi, mu=0, sigma=1):\n",
    "    return 1-normal_prob_between(lo, hi, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do the reverse—find either the nontail region or the (symmetric) interval\n",
    "around the mean that accounts for a certain level of likelihood. For example, if we\n",
    "want to find an interval centered at the mean and containing 60% probability, then\n",
    "we find the cutoffs where the upper and lower tails each contain 20% of the probability (leaving 60%):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_normal_cdf(p, mu=0, sigma=1, tolerance=0.00001):\n",
    "    \"\"\"Find approx. inverse using binary search.\"\"\"\n",
    "    \n",
    "    # if not standard, compute standard and rescale\n",
    "    if mu != 0 and sigma != 1:\n",
    "        return mu+sigma*inverse_normal_cdf(p, tolerance=tolerance)\n",
    "    \n",
    "    low_z, low_p = -10.0, 0  # normal_cdf(-10) is (very close to) 0\n",
    "    hi_z, hi_p = 10.0, 0     # normal_cdf(10) is (very close to) 1\n",
    "    \n",
    "    while hi_z - low_z > tolerance:\n",
    "        mid_z = (low_z + hi_z)/2  # Consider the midpoint.\n",
    "        mid_p = normal_cdf(mid_z) # and the cdf's value there\n",
    "        \n",
    "        if mid_p < p:\n",
    "            # midpoint is still too low, search above it\n",
    "            low_z, low_p = mid_z, mid_p\n",
    "        elif mid_p > p:\n",
    "            # midpoint is still too high, search below it\n",
    "            hi_z, hi_p = mid_z, mid_p\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return mid_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_upper_bound(probability, mu=0, sigma=1):\n",
    "    \"\"\"Return the z for which P(Z<=z) = probability.\"\"\"\n",
    "    return inverse_normal_cdf(probability, mu, sigma)\n",
    "\n",
    "def normal_lower_bound(probability, mu=0, sigma=1):\n",
    "    \"\"\"Returns the z for which P(Z>=z) = probability.\"\"\"\n",
    "    return inverse_normal_cdf(1-probability, mu, sigma)\n",
    "\n",
    "def normal_two_sided_bounds(probability, mu=0, sigma=1):\n",
    "    \"\"\"Returns the symmetric (about the mean) bounds that \n",
    "       contain the specified probability.\"\"\"\n",
    "    tail_probability = (1-probability)/2\n",
    "    \n",
    "    # upper bound should have tail_probability above it\n",
    "    upper_bound = normal_lower_bound(tail_probability, mu, sigma)\n",
    "    \n",
    "    # lower bound should have tail_probability below it\n",
    "    lower_bound = normal_upper_bound(tail_probability, mu, sigma)\n",
    "    \n",
    "    return lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, let’s say that we choose to flip the coin n = 1000 times. If our hypothesis\n",
    "of fairness is true, X (number of heads in n-times coin flipping) should be distributed approximately normally with mean 500 and standard deviation 15.8. Such result can be confirmed from normal approximation of binomial distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu_0 = 500.0 , sigma_0 = 15.811388300841896\n"
     ]
    }
   ],
   "source": [
    "mu_0, sigma_0 = normal_approx_to_binomial(1000, 0.5)\n",
    "print('mu_0 =',mu_0, ', sigma_0 =',sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ordere to design statistical test, two things must be considered:\n",
    "\n",
    "1. *Threshold of test*. Null hypothesis $H_0$ can be rejected when X is observed to be outside of certain thereshold.\n",
    "2. *Power of test*. The test should be design such that the probability of fail to reject null hypothesis $H_0$, evethough it's false, is small.\n",
    "\n",
    "\n",
    "To determine the threshold, we need to make a decision about *__significance—how willing we are to make a type 1\n",
    "error (“false positive”), in which we reject $H_{0}$ even though it’s true__*. For reasons lost to\n",
    "the annals of history, this willingness is often set at 5% or 1%. Let’s choose 5%.\n",
    "\n",
    "Consider the test that rejects $H_{0}$ if X falls outside the bounds given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(469.01026640487555, 530.9897335951244)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_two_sided_bounds(0.95, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Assuming p really equals 0.5 (i.e., $H_{0}$ is true), there is just a 5% chance we observe an\n",
    "X that lies outside this interval, which is the exact significance we wanted__*. Said differently, if $H_{0}$ is true, then, approximately 19 times out of 20, this test will give the correct result.\n",
    "\n",
    "*__The power of a test, which is the probability of not\n",
    "making a type 2 error, in which we fail to reject $H_{0}$ even though it’s false__*. In order to\n",
    "measure this, we have to specify what exactly $H_{0}$ being false means. (Knowing merely\n",
    "that p is not 0.5 doesn’t give you a ton of information about the distribution of X.) In\n",
    "particular, let’s check what happens if p is really 0.55, so that the coin is slightly biased\n",
    "toward heads.\n",
    "\n",
    "In that case, we can calculate the power of the test with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95% bounds based on assumption p is 0.5\n",
    "lo, hi = normal_two_sided_bounds(0.95, mu_0, sigma_0)\n",
    "\n",
    "# Actual mu and sigma based on p = 0.55\n",
    "mu_1, sigma_1 = normal_approx_to_binomial(1000, 0.55)\n",
    "\n",
    "# A type 2 error means we fail to reject the null hypothesis\n",
    "# which will happen when X is still in our original interval.\n",
    "type_2_probability = normal_prob_between(lo, hi, mu_1, sigma_1)\n",
    "power = 1 - type_2_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8865480012953671\n"
     ]
    }
   ],
   "source": [
    "print(power)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine instead that our null hypothesis $H_0$ was that the coin is not biased toward heads,\n",
    "or that $p \\leq 0.5$ . In that case we want a *one-sided test* that rejects the null hypothesis\n",
    "when X is much larger than 500 but not when X is smaller than 500. So a 5% significance test involves using normal_probability_below to find the cutoff below which 95% of the probability lies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = normal_upper_bound(0.95, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526.0073585242053\n"
     ]
    }
   ],
   "source": [
    "print(hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_2_prob = normal_prob_below(hi, mu_1, sigma_1)\n",
    "power = 1 - type_2_prob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9363794803307174\n"
     ]
    }
   ],
   "source": [
    "print(power)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a more powerful test, since it no longer rejects $H_0$ when X is below 469 (which\n",
    "is very unlikely to happen if $H_1$ is true) and instead rejects $H_0$ when X is between 526\n",
    "and 531 (which is somewhat likely to happen if $H_1$ is true).\n",
    "\n",
    "An alternative way of thinking about the preceding test involves *p-values*. Instead of\n",
    "choosing bounds based on some probability cutoff, *we compute the probability—\n",
    "assuming $H_0$ is true—that we would see a value at least as extreme as the one we\n",
    "actually observed*.\n",
    "\n",
    "For our two-sided test of whether the coin is fair, we compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_sided_pvalue(x, mu=0, sigma=1):\n",
    "    if x >= mu:\n",
    "        # if x is greater than the mean, the tail is what's greater than x\n",
    "        return 2*normal_prob_above(x, mu, sigma)\n",
    "    else:\n",
    "        # if x is less than the mean, the tail is what's less than x\n",
    "        return 2*normal_prob_below(x, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to see 530 heads, we would compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06207721579598835"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sided_p_value(529.5, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did we use 529.5 instead of 530? This is what’s called a continuity correction. It reflects the fact that normal_probability_between(529.5, 530.5, mu_0, sigma_0) is a better estimate\n",
    "of the probability of seeing 530 heads than normal_probability_between(530, 531, mu_0, sigma_0) is.\n",
    "\n",
    "Correspondingly, normal_probability_above(529.5, mu_0,\n",
    "sigma_0) is a better estimate of the probability of seeing at least\n",
    "530 heads. \n",
    "\n",
    "One way to convince yourself that this is a sensible estimate is with a simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06123\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "extreme_value_count = 0\n",
    "\n",
    "for _ in range(100000):\n",
    "    num_heads = sum(1 if random.random()<0.5 else 0 \n",
    "                    for _ in range(1000)) # Count number of Heads in 1000 flips.\n",
    "    if num_heads >= 530 or num_heads <= 470:\n",
    "        extreme_value_count += 1          # Count how often the number of Heads is extreme.\n",
    "        \n",
    "print(extreme_value_count/100000)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the p-value is greater than our 5% significance, we don’t reject the null. If we\n",
    "instead saw 532 heads, the p-value would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.046345287837786575"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sided_pvalue(531.5, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is smaller than the 5% significance, which means we would reject the null. It’s\n",
    "the exact same test as before. It’s just a different way of approaching the statistics.\n",
    "\n",
    "Similarly, we would have: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_pvalue = normal_prob_above\n",
    "lower_pvalue = normal_prob_below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one-sided test, if we saw 525 heads we would compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06062885772582072"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_pvalue(524.5, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which means we wouldn’t reject the null. If we saw 527 heads, the computation would\n",
    "be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04686839508859242"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_pvalue(526.5, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we would reject the null.\n",
    "\n",
    "Make sure your data is roughly normally distributed before using\n",
    "normal_probability_above to compute p-values. The annals of\n",
    "bad data science are filled with examples of people opining that the\n",
    "chance of some observed event occurring at random is one in a\n",
    "million, when what they really mean is “the chance, assuming the\n",
    "data is distributed normally,” which is pretty meaningless if the data\n",
    "isn’t.\n",
    "\n",
    "There are various statistical tests for normality, but even plotting\n",
    "the data is a good start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve been testing hypotheses about the value of the heads probability p, which is a\n",
    "parameter of the unknown “heads” distribution. When this is the case, a third\n",
    "approach is to construct a confidence interval around the observed value of the\n",
    "parameter.\n",
    "\n",
    "For example, we can estimate the probability of the unfair coin by looking at the aver‐\n",
    "age value of the Bernoulli variables corresponding to each flip—1 if heads, 0 if tails. If\n",
    "we observe 525 heads out of 1,000 flips, then we estimate p equals 0.525.\n",
    "\n",
    "How confident can we be about this estimate? Well, if we knew the exact value of p,\n",
    "the central limit theorem (recall “The Central Limit Theorem” on page 78) tells us\n",
    "that the average of those Bernoulli variables should be approximately normal, with\n",
    "mean p and standard deviation:\n",
    "\n",
    "    math.sqrt(p*(1-p)/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we don’t know p, so instead we use our estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu= 0.525 , sigma= 0.015791611697353755\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "p_hat = 525/1000\n",
    "mu = p_hat\n",
    "sigma = math.sqrt(p_hat*(1-p_hat)/1000)\n",
    "\n",
    "print('mu=',mu,', sigma=',sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not entirely justified, but people seem to do it anyway. Using the normal\n",
    "approximation, we conclude that we are “95% confident” that the following interval\n",
    "contains the true parameter p:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4940490278129096, 0.5559509721870904)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_two_sided_bounds(0.95, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a statement about the interval, not about p. You should\n",
    "understand it as the assertion that if you were to repeat the experiment many times, 95% of the time the “true” parameter (which is\n",
    "the same every time) would lie within the observed confidence\n",
    "interval (which might be different every time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, we do not conclude that the coin is unfair, since 0.5 falls within our con‐\n",
    "fidence interval.\n",
    "\n",
    "If instead we’d seen 540 heads, then we’d have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu= 0.54 , sigma= 0.015760710643876435\n"
     ]
    }
   ],
   "source": [
    "p_hat = 540/1000\n",
    "mu = p_hat\n",
    "sigma = math.sqrt(p_hat*(1-p_hat)/1000)\n",
    "\n",
    "print('mu=',mu,', sigma=',sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5091095927295919, 0.5708904072704082)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_two_sided_bounds(0.95, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, “fair coin” doesn’t lie in the confidence interval. (The “fair coin” hypothesis\n",
    "doesn’t pass a test that you’d expect it to pass 95% of the time if it were true.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P-hacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A procedure that erroneously rejects the null hypothesis only 5% of the time will—by\n",
    "definition—5% of the time erroneously reject the null hypothesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "def run_exp():\n",
    "    \"\"\"Flip a fair coin 1000 times. True == Heads, False == Tail.\"\"\"\n",
    "    return [random.random() < 0.5 for _ in range(1000)]\n",
    "\n",
    "def reject_fairness(experiment):\n",
    "    \"\"\"using the 5% significance levels\"\"\"\n",
    "    num_heads = len([flip for flip in experiment if flip])\n",
    "    return num_heads < 469 or num_heads > 531\n",
    "\n",
    "random.seed(0)\n",
    "experiments = [run_exp() for _ in range(1000)]\n",
    "num_rejections = len([experiment for experiment in experiments if reject_fairness(experiment)])\n",
    "\n",
    "print(num_rejections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this means is that if you’re setting out to find “significant” results, you usually\n",
    "can. Test enough hypotheses against your data set, and one of them will almost certainly appear significant. Remove the right outliers, and you can probably get your p\n",
    "value below 0.05.\n",
    "\n",
    "This is sometimes called P-hacking and is in some ways a consequence of the “infer‐\n",
    "ence from p-values framework.”\n",
    "\n",
    "*__If you want to do good science, you should determine your hypotheses before looking\n",
    "at the data, you should clean your data without the hypotheses in mind, and you\n",
    "should keep in mind that p-values are not substitutes for common sense__*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Running an A/B Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of your primary responsibilities at DataSciencester is experience optimization,\n",
    "which is a euphemism for trying to get people to click on advertisements. One of\n",
    "your advertisers has developed a new energy drink targeted at data scientists, and the\n",
    "VP of Advertisements wants your help choosing between advertisement A (“tastes\n",
    "great!”) and advertisement B (“less bias!”).\n",
    "\n",
    "Being a scientist, you decide to run an experiment by randomly showing site visitors\n",
    "one of the two advertisements and tracking how many people click on each one.\n",
    "\n",
    "If 990 out of 1,000 A-viewers click their ad while only 10 out of 1,000 B-viewers click\n",
    "their ad, you can be pretty confident that A is the better ad. But what if the differences\n",
    "are not so stark? Here’s where you’d use statistical inference.\n",
    "\n",
    "Let’s say that $N_A$ people see ad A, and that $n_A$ of them click it. We can think of each ad\n",
    "view as a Bernoulli trial where $p_A$ is the probability that someone clicks ad A. Then (if\n",
    "$N_A$ is large, which it is here) we know that $n_A / N_A$ is approximately a normal random\n",
    "variable with mean $p_A$ and standard deviation $\\sigma_A = \\sqrt{p_A (1-p_A)/N_A}$.\n",
    "\n",
    "Similarly, $n_B / N_B$ is approximately a normal random variable with mean $p_B$ and standard deviation $\\sigma_B = \\sqrt{p_B (1-p_B)/N_B}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimated_param(N,n):\n",
    "    p = n/N\n",
    "    sigma = math.sqrt(p*(1-p)/N)\n",
    "    return p,sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume those two normals are independent (which seems reasonable, since the\n",
    "individual Bernoulli trials ought to be), then their difference should also be normal\n",
    "with mean $p_B - p_A$ and standard deviation $\\sqrt{\\sigma_A^2 + \\sigma_B^2}$. \n",
    "\n",
    "This is sort of cheating. The math only works out exactly like this if\n",
    "you know the standard deviations. Here we’re estimating them\n",
    "from the data, which means that we really should be using a tdistribution. But for large enough data sets, it’s close enough that it doesn’t make much of a difference.\n",
    "\n",
    "This means we can test the null hypothesis that $p_A$ and $p_B$ are the same (that is, that\n",
    "$p_B - p_A$ is zero), using the statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_b_statistics(N_A, n_A, N_B, n_B):\n",
    "    p_A, sigma_A = estimated_param(N_A, n_A)\n",
    "    p_B, sigma_B = estimated_param(N_B, n_B)\n",
    "    return (p_B - p_A)/math.sqrt(sigma_A**2 + sigma_B**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which should approximately be a standard normal.\n",
    "\n",
    "For example, if “tastes great” gets 200 clicks out of 1,000 views and “less bias” gets 180\n",
    "clicks out of 1,000 views, the statistic equals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.1403464899034472\n"
     ]
    }
   ],
   "source": [
    "z = a_b_statistics(1000, 200, 1000, 180)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of seeing such a large difference if the means were actually equal\n",
    "would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.254141976542236"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sided_pvalue(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is large enough that you can’t conclude there’s much of a difference. On the\n",
    "other hand, if “less bias” only got 150 clicks, we’d have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.948839123097944\n"
     ]
    }
   ],
   "source": [
    "z = a_b_statistics(1000, 200, 1000, 150)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003189699706216853"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sided_pvalue(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which means there’s only a 0.003 probability you’d see such a large difference if the\n",
    "ads were equally effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedures we’ve looked at have involved making probability statements about\n",
    "our tests: “there’s only a 3% chance you’d observe such an extreme statistic if our null\n",
    "hypothesis were true.”\n",
    "\n",
    "An alternative approach to inference involves treating the unknown parameters\n",
    "themselves as random variables. The analyst (that’s you) starts with a prior distribution for the parameters and then uses the observed data and Bayes’s Theorem to get\n",
    "an updated posterior distribution for the parameters. *__Rather than making probability\n",
    "judgments about the tests, you make probability judgments about the parameters\n",
    "themselves__*.\n",
    "\n",
    "For example, when the unknown parameter is a probability (as in our coin-flipping\n",
    "example), we often use a prior from the Beta distribution, which puts all its probabil‐\n",
    "ity between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def B(alpha, beta):\n",
    "    \"\"\"a normalizing constant so that the total probability is 1\"\"\"\n",
    "    return math.gamma(alpha) * math.gamma(beta) / math.gamma(alpha + beta)\n",
    "\n",
    "def beta_pdf(x, alpha, beta):\n",
    "    if x < 0 or x > 1: # no weight outside of [0, 1]\n",
    "        return 0\n",
    "    return x ** (alpha - 1) * (1 - x) ** (beta - 1) / B(alpha, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, this distribution centers its weight at:\n",
    "\n",
    "    alpha / (alpha + beta)\n",
    "    \n",
    "and the larger alpha and beta are, the “tighter” the distribution is.\n",
    "\n",
    "For example, if alpha and beta are both 1, it’s just the uniform distribution (centered\n",
    "at 0.5, very dispersed). If alpha is much larger than beta, most of the weight is near 1.\n",
    "And if alpha is much smaller than beta, most of the weight is near zero. Figure 7-1\n",
    "shows several different Beta distributions.\n",
    "\n",
    "So let’s say we assume a prior distribution on p. Maybe we don’t want to take a stand\n",
    "on whether the coin is fair, and we choose alpha and beta to both equal 1. Or maybe\n",
    "we have a strong belief that it lands heads 55% of the time, and we choose alpha\n",
    "equals 55, beta equals 45.\n",
    "\n",
    "Then we flip our coin a bunch of times and see h heads and t tails. Bayes’s Theorem\n",
    "(and some mathematics that’s too tedious for us to go through here) tells us that the\n",
    "posterior distribution for p is again a Beta distribution but with parameters alpha +\n",
    "h and beta + t.\n",
    "\n",
    "It is no coincidence that the posterior distribution was again a Beta\n",
    "distribution. The number of heads is given by a Binomial distribu‐\n",
    "tion, and the Beta is the conjugate prior to the Binomial distribu‐\n",
    "tion. This means that whenever you update a Beta prior using\n",
    "observations from the corresponding binomial, you will get back a\n",
    "Beta posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](beta_dist.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s say you flip the coin 10 times and see only 3 heads.\n",
    "If you started with the uniform prior (in some sense refusing to take a stand about\n",
    "the coin’s fairness), your posterior distribution would be a Beta(4, 8), centered around\n",
    "0.33. Since you considered all probabilities equally likely, your best guess is some‐\n",
    "thing pretty close to the observed probability.\n",
    "\n",
    "If you started with a Beta(20, 20) (expressing the belief that the coin was roughly\n",
    "fair), your posterior distribution would be a Beta(23, 27), centered around 0.46, indi‐\n",
    "cating a revised belief that maybe the coin is slightly biased toward tails.\n",
    "\n",
    "And if you started with a Beta(30, 10) (expressing a belief that the coin was biased to\n",
    "flip 75% heads), your posterior distribution would be a Beta(33, 17), centered around\n",
    "0.66. In that case you’d still believe in a heads bias, but less strongly than you did ini‐\n",
    "tially. These three different posteriors are plotted in Figure 7-2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](post_dist.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you flipped the coin more and more times, the prior would matter less and less\n",
    "until eventually you’d have (nearly) the same posterior distribution no matter which\n",
    "prior you started with.\n",
    "\n",
    "For example, no matter how biased you initially thought the coin was, it would be\n",
    "hard to maintain that belief after seeing 1,000 heads out of 2,000 flips (unless you are\n",
    "a lunatic who picks something like a Beta(1000000,1) prior).\n",
    "\n",
    "What’s interesting is that this allows us to make probability statements about hypotheses: “Based on the prior and the observed data, there is only a 5% likelihood the\n",
    "coin’s heads probability is between 49% and 51%.” This is philosophically very different from a statement like “if the coin were fair we would expect to observe data so\n",
    "extreme only 5% of the time.”\n",
    "\n",
    "Using Bayesian inference to test hypotheses is considered somewhat controversial—\n",
    "in part because its mathematics can get somewhat complicated, and in part because of\n",
    "the subjective nature of choosing a prior. We won’t use it any further in this book, but\n",
    "it’s good to know about."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
