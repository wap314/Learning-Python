{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9 - A Simplistic Introduction to Algorithmic Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wriring efficient program is not easy. The most straightforward solution is often not the most efficient. Computationally efficient algorithms often employ subtle tricks that can make them difficult to understand.Consequently, programmers often increase the conceptual complexity of a program in an effort to reduce its computational complexity. To do this in a sensible way, we need to understand how to go about estimating the computational complexity of a program ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Thinking About Computational Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should one go about answering the question \"How long will the following function take to run ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(i):\n",
    "    \"\"\"Assumes that i>=0 is an integer.\"\"\"\n",
    "    ans = 1\n",
    "    while i>=1:\n",
    "        ans *= i\n",
    "        i -= 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run the program on some particular input and time it, then it would not be informative because the result would depend upon:\n",
    "\n",
    "* the speed of the computer on which it is run.\n",
    "* the efficiency of the Python implementation on that machine.\n",
    "* the value of the input.\n",
    "\n",
    "We can get around the first two issues by using a more abstract measure of time. We measure time in terms of the number of basic steps executed by the program. \n",
    "\n",
    "For simplicity, we will use a random access machine as our model of computation. In a random acccess machine, steps are executed sequentially, one at a time. A step is an operation that takes a fixed amount of time, such as binding a variable to an object, making a comparison, executing an arithmetic operation, or accessing an object in memory.\n",
    "\n",
    "Now we turn to the question of dependence on the value of the input. We deal with that by moving away from expressing time complexity as a single number and instead relating it to the sizes of the inputs. This allow us to compare the efficiency of two algorithms by talking about how the running time of each grows with respect to the sizes of the inputs.\n",
    "\n",
    "Of course, the actual running time of an algorithm depends not only upon the sizes of the inputs but also upon their values. Consider, for example, the linear search algorithm implemented by : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearSearch(L,x):\n",
    "    for e in L:\n",
    "        if e == x:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [1,2,3,4,5]\n",
    "x = 9\n",
    "\n",
    "linearSearch(L,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that L is a list containing a million elements, and consider the call\n",
    "linearSearch(L, 3). If the first element in L is 3, linearSearch will return True almost immediately. On the other hand, if 3 is not in L, linearSearch will have to\n",
    "examine all one million elements before returning False.\n",
    "\n",
    "In general, there are three broad cases to think about:\n",
    "\n",
    "* The best-case running time is the running time of the algorithm when the inputs are as favorable as possible. I.e. the best case running-time is the minimum running time over all the possible inputs of a given size. For linearSearch, the best-case running time is independent of the size of L.\n",
    "* Similarly, the worst-case running time is the maximum running time over all the possible inputs of a given size. For linearSearch, the worst-case running time is linear in the size of L.\n",
    "* The average-case (expected-case) running time is the average running-time over all possible inputs of a given size. Alternatively, if one has some a priori information about the distribution of input values (e.g. tha 90% of the time x is in L), one can take that into account.\n",
    "\n",
    "People usually focus on the worst-case. The worst-case provide an upper bound on the running time. This is critical in situations where there is a time constraints on how long a computation can take.\n",
    "\n",
    "Let's look at the worst-case running time of an iterative implementation of the factorial function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fact(n):\n",
    "    \"\"\"Assumes n>0 is natural number. Return n! .\"\"\"\n",
    "    ans = 1      #1 step\n",
    "    while n>1:   #n-times iteration\n",
    "        ans *= n #2 steps\n",
    "        n -= 1   #2 steps \n",
    "    return ans   #1 step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of steps required to run fact(n) is something like 2+5n. So, for example, if n=1000, the function will execute roughly  5002 steps. \n",
    "\n",
    "It shoud be obvious that as n gets large, worrying about the difference between 5n and 5n+2 is a kind of silly. For this reason, we typically ignore additive constants when reasoning about running time. Multiplicative factor, however, can be important.\n",
    "\n",
    "On the other hand, when one is comparing two different algorithm, it is often the case that even multiplicative constants are irrelevant. Recall two algorithms, exhaustive enumeration and bisection search, for finding an approximation to the square root of a floating point number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using exhaustive enumeration to approximate square root.\n",
    "def squareRootExhaustive(x,epsilon):\n",
    "    \"\"\"Assumes x>0 and 0<epsilon<1 are floats. \n",
    "       Return a y s.t. y*y is within epsilon of x.\"\"\"\n",
    "    step = epsilon**2\n",
    "    ans = 0.0\n",
    "    numIter = 0\n",
    "    \n",
    "    while abs(ans**2-x) >= epsilon and ans**2 <= x :\n",
    "        ans += step\n",
    "        numIter += 1\n",
    "    if ans**2 > x:\n",
    "        raise ValueError\n",
    "    return numIter, ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using bisection search to approximate square root.\n",
    "def squareRootBi(x,epsilon):\n",
    "    \"\"\"Assumes x>0 and 0<epsilon<1 are floats. \n",
    "       Return a y s.t. y*y is within epsilon of x.\"\"\"\n",
    "    low = 0.0\n",
    "    high = max(1.0,x)\n",
    "    ans = (low + high)/2.0\n",
    "    numIter = 0\n",
    "    \n",
    "    while abs(ans**2-x) >= epsilon :\n",
    "        if ans**2 < x:\n",
    "            low = ans\n",
    "        else:\n",
    "            high = ans\n",
    "        ans = (low + high)/2.0\n",
    "        numIter += 1\n",
    "    return numIter, ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99995, 9.999499999990034)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squareRootExhaustive(100, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 9.999847412109375)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squareRootBi(100, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that exhaustive enumeration was so slow as to be impractical for many combinations of x and epsilon. When the difference in the number of iteration is this large, it doesn't really matter how many instructions are in the loop, i.e. the multiplicative constants are irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Asymptotic Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The asymptotic notation is used to provide a formal way to talk about the relationship between the running time of an algorithm and the size of its inputs. The underlying motivation is that almost any algorithm is sufficiently efficient when run on small inputs. What we typically need to worry about is the efficiency of the algorithm when run on very large inputs. As a proxy for \"very large\", asymptotic notation describes the complexity of an algorithm as the size of its inputs approaches infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"Assumes that x>0 is an integer.\"\"\"\n",
    "    ans = 0\n",
    "    \n",
    "    #Loop that takes constant time.\n",
    "    for i in range(1000):\n",
    "        ans += 1\n",
    "    print('Number of additions so far', ans)\n",
    "    \n",
    "    #Loop that takes time x.\n",
    "    for i in range(x):\n",
    "        ans += 1\n",
    "    print('Number of additions so far', ans)\n",
    "    \n",
    "    #Nested loop take time x**2\n",
    "    for i in range(x):\n",
    "        for j in range(x):\n",
    "            ans += 1\n",
    "            ans += 1\n",
    "    print('Number of additions so far', ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one assumes that each line of code takes one unit of time to execute, the running time of this function can be described as 1000+x+2x^2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of additions so far 1000\n",
      "Number of additions so far 1010\n",
      "Number of additions so far 1210\n"
     ]
    }
   ],
   "source": [
    "f(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of additions so far 1000\n",
      "Number of additions so far 2000\n",
      "Number of additions so far 2002000\n"
     ]
    }
   ],
   "source": [
    "f(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For small values of x the constant term dominates. However, for large x the square term dominates. This kind of analysis lead us to use the following rules of thumb in describing the asymptotic complexity of an algorithm:\n",
    "\n",
    "* If the running time is the sum of multiple terms, keep the one with the largest growth rate and drop the others.\n",
    "* If the remaining term is a product, drop any constants.\n",
    "\n",
    "The most commonly used asymptotic notation is called \"Big O\" notation. Big O notation is used to give an upper bound on the asymptotic growth of a function. For example, the formula f(x) is an element of O(x^2) means that the function f grows no faster than the quadratic polynomial x^2, in an asymptotic sense.\n",
    "\n",
    "We will often abuse Big O notation by making statements like \"the complexity of f(x) is O(x^2)\". By this we mean that in the worst case f will take O(x^2) steps to run. The difference between a function being \"in O(x^2)\" and \"being O(x^2)\" is subtle but important. Saying that f(x) in O(x^2) does not preclude the worst-case running time of f from being considerably less than O(x^2).\n",
    "\n",
    "When we say that f(x) is O(x^2), we are implying that x^2 is both an upper and a lower bound on the asymptotic worst-case running time. This is called tight bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Some Important Complexity Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the most common instances of Big O are listed below. In each case, n is a measure of the size of the inputs to the function.\n",
    "\n",
    "*  O(1) denotes constant running time.\n",
    "* O(log\tn) denotes logarithmic running time.\n",
    "* O(n) denotes linear running time.\n",
    "* O(n\tlog\tn) denotes log-linear running time.\n",
    "* O(n^k) denotes polynomial running time. Notice that k is a constant.\n",
    "* O(c^n) denotes exponential running time. Here a constant is being raised to a\n",
    "power based on the size of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.1 Constant Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates that the asymptotic complexity is independent of the size of the\n",
    "inputs. There are very few interesting programs in this class, but all programs\n",
    "have pieces (for example finding out the length of a Python list or multiplying\n",
    "two floating point numbers) that fit into this class. Constant running time does\n",
    "not imply that there are no loops or recursive calls in the code, but it does imply\n",
    "that the number of iterations or recursive calls is independent of the size of the\n",
    "inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.2 Logarithmic Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such function have a complexity that grows as the log of at least one of the inputs. We don't care about the base of the log, since the difference between one base and another is merely a constant multiplicative factor. Consider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intToStr(i):\n",
    "    \"\"\"Assumes i>=0 is an integer. \n",
    "    Returns a decimal string representation of i.\"\"\"\n",
    "    digits = '0123456789'\n",
    "    if i == 0:\n",
    "        return '0'\n",
    "    result = ''\n",
    "    while i > 0:\n",
    "        result = digits[i%10] + result\n",
    "        i = i//10\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'68'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intToStr(68)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are no function or method calls in this code, we know that we only have to look at the loops to determine the complexity class. There is only one loop, so the only thing we need to do is characterize the number of iterations. That boils down to the number of times we can use integer division to divide i by 10 before getting result of 0. So, the complexity of intToStr is O(log(i)). What about the complexity of :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDigits(n):\n",
    "    \"\"\"Assumes n>=0 is an integer.\n",
    "       Returns the sum of the digits in n.\"\"\"\n",
    "    stringRep = intToStr(n) #string rep. of a given integer with complexity O(log(n))\n",
    "    val = 0\n",
    "    for c in stringRep:  \n",
    "        val += int(c)    \n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "a\n",
      "b\n",
      "c\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "val = '1abc5'\n",
    "\n",
    "for i in val:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complexity of converting n to a string using intToStr is O(log(n)) and intToStr returns a string of length O(log(n)). The for loop will be executed O(len(stringRep)) times, i.e. O(log(n)) times. Assume that a character can be converted to a digitin constant time, the program will run in time proportional to O(log(n)) + O(log(n)), which makes it O(log(n))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.3 Linear Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many algorithms that deal with list or other kinds of sequences are linear because they touch each element of the sequence a constant (greater than 0) number of times. Consider, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDigits(s):\n",
    "    \"\"\"Assumes s is a str, each character of which is a decimal digit. \n",
    "       Return an integer that is the sum of the digits in s.\"\"\"\n",
    "    val = 0\n",
    "    for c in s:\n",
    "        val += int(c)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addDigits('12345')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus function is linear in the length of s, i.e. O(len(s)), again assuming that a character representing a digit can be converted to an integer in constant time. Of course, a program does not need to have a loop to have linear complexity. Consider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorial(x):\n",
    "    \"\"\"Assumes that x>0 is an integer. Returns x! .\"\"\"\n",
    "    if x == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return x*factorial(x-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no loops in this code, so in order to analyze the complexity we need to figure out how many recursive calls get made. The series of calls is simply:\n",
    "\n",
    "    factorial(x), factorial(x-1), ..., factorial(2), factorial(1)\n",
    "    \n",
    "The length of this series, and thus the complexity of the function, is O(x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.4 Log-Linear Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is slightly more complicated than the complexity classes we have looked at\n",
    "thus far. It involves the product of two terms, each of which depends upon the\n",
    "size of the inputs. It is an important class, because many practical algorithms are\n",
    "log-linear. The most commonly used log-linear algorithm is probably merge sort,\n",
    "which is O(n\tlog(n)), where n is the length of the list being sorted. We will look at\n",
    "that algorithm and analyze its complexity in Chapter 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.5 Polynomial Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most commonly used polynomial algorithms are quadratic, i.e. their complexity grows as the square of the size of their input. Consider, for example the following function which implement a subset test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isSubset(L1,L2):\n",
    "    \"\"\"Assumes L1 and L2 are lists. Returns True if each \n",
    "       element in L1 is also in L2, and False otherwise.\"\"\"\n",
    "    for e1 in L1:\n",
    "        matched = False\n",
    "        for e2 in L2:\n",
    "            if e1 == e2:\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isSubset([1,2,3],[4,5,6,1,2,3,7,8,9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time the inner loop is reached it is executed O(len(L2) times. The function isSubset will execute the outer loop O(len(L1)) times, so the inner loop will\n",
    "be reached O(len(L1)) times. Therefore, the complexity of the function isSubset\n",
    "is O(len(L1)* len(L2)). Now consider the function intersect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect(L1,L2):\n",
    "    \"\"\"Assumes both L1 and L2 are lists. Return a list without duplicates that is the intersection of L1 and L2.\"\"\"\n",
    "    \n",
    "    #Build a list containing common elements.\n",
    "    tmp = []\n",
    "    for e1 in L1:\n",
    "        for e2 in L2:\n",
    "            if e1 == e2:\n",
    "                tmp.append(e1)\n",
    "                break\n",
    "    \n",
    "    #Build a list without duplicates\n",
    "    result  = []\n",
    "    for e in tmp:\n",
    "        if e not in result:\n",
    "            result.append(e)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The running time for the part building the list that might contain duplicates\n",
    "is clearly O(len(L1)* len(L2)). At first glance, it appears that the part of the code\n",
    "that builds the duplicate-free list is linear in the length of tmp, but it is not. The\n",
    "test e not in result potentially involves looking at each element in result, and is\n",
    "therefore O(len(result)); consequently the second part of the implementation is\n",
    "O(len(tmp)* len(result)). However, since the lengths of result and tmp are bounded by the length of the smaller of L1 and L2, and since we ignore additive terms,\n",
    "the complexity of intersect is O(len(L1)*len(L2))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.6 Exponential Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many importants problems are inherently exponential, i.e. solving them completely can require time that is exponential inthe size of the input. This is unfortunate, since it rarely pays to write a program that has a reasonably high probability of taking exponential time to run. Consider, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBinaryRep(n, numDigits):\n",
    "    \"\"\"Assumes n and numDigits are non-negative integers. Returns\n",
    "       a str of length numDigits that is a binary rep. of n.\"\"\"\n",
    "    \n",
    "    result = ''\n",
    "    while n>0: #O(n)\n",
    "        result = str(n%2) + result #either 0 (n is even) or 1 (n is odd).\n",
    "        n = n//2 #stair case division\n",
    "    if len(result) > numDigits:\n",
    "        raise ValueError('not enough digits.')\n",
    "    for i in range(numDigits-len(result)): #O(numDigits)\n",
    "        result = '0' + result\n",
    "    return result\n",
    "\n",
    "def genPowerset(L):\n",
    "    \"\"\"Assumes L is a list. Returns a list of lists that contains all possible \n",
    "       combinations of the elements of L.  E.g., if L is [1, 2] it will return \n",
    "       a list with elements [], [1], [2], and [1,2]\"\"\"\n",
    "    powerset = []\n",
    "    for i in range(0,2**len(L)): #O(2**len(L))\n",
    "        binStr = getBinaryRep(i, len(L))\n",
    "        subset = []\n",
    "        for j in range(len(L)): #O(len(L))\n",
    "            if binStr[j] == '1':\n",
    "                subset.append(L[j])\n",
    "        powerset.append(subset)\n",
    "    return powerset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = ['a','b','c','d','e','f','g','h','i','j']\n",
    "genPowerset([L])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function genPowerset(L) returns a list of lists that contains all possible\n",
    "combinations of the elements of L. For example, if L is ['x', 'y'], the powerset of\n",
    "L will be a list containing the lists [], ['x'], ['y'], and ['x', 'y'].\n",
    "The algorithm is a bit subtle. Consider a list of n elements. We can represent\n",
    "any combination of elements by a string of n 0’s and 1’s, where a 1 represents the\n",
    "presence of an element and a 0 its absence. The combination containing no items\n",
    "is represented by a string of all 0’s, the combination containing all of the items is\n",
    "represented by a string of all 1’s, the combination containing only the first and\n",
    "last elements is represented by 100…001, etc\n",
    "\n",
    "Generating all sublists of a list L of length n can be done as follows:\n",
    "\n",
    "* Generate all n-bit binary numbers. These are the numbers from 0 to 2n.\n",
    "* For each of these 2n +1 binary numbers, b, generate a list by selecting those\n",
    "elements of L that have an index corresponding to a 1 in b. For example, if L is\n",
    "['x', 'y'] and b is 01, generate the list ['y'].\n",
    "\n",
    "Try running genPowerset on a list containing the first ten letters of the alphabet. It will finish quite quickly and produce a list with 1024 elements. Next, try\n",
    "running genPowerset on the first twenty letters of the alphabet. It will take more\n",
    "than a bit of time to run, and return a list with about a million elements. If you\n",
    "try running genPowerset on all twenty-six letters, you will probably get tired of\n",
    "waiting for it to complete, unless your computer runs out of memory trying to\n",
    "build a list with tens of millions of elements. Don’t even think about trying to run\n",
    "genPowerset on a list containing all uppercase and lowercase letters. Step 1 of the\n",
    "algorithm generates O(2len(L)) binary numbers, so the algorithm is exponential in\n",
    "len(L).\n",
    "\n",
    "Does this mean that we cannot use computation to tackle exponentially hard\n",
    "problems? Absolutely not. It means that we have to find algorithms that provide\n",
    "approximate solutions to these problems or that find perfect solutions on some\n",
    "instances of the problem. But that is a subject for later chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3.7 Comparisons of Complexity Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots in this section are intended to convey an impression of the implications\n",
    "of an algorithm being in one or another of these complexity classes.\n",
    "The plot on the left in Figure 9.7 compares the growth of a constant-time algorithm to that of a logarithmic algorithm. Note that the size of the input has to\n",
    "reach about a million for the two of them to cross, even for the very small constant of twenty. When the size of the input is five million, the time required by a\n",
    "logarithmic algorithm is still quite small. The moral is that logarithmic algorithms are almost as good as constant-time ones.\n",
    "The plot on the right of Figure 9.7 illustrates the dramatic difference between logarithmic algorithms and linear algorithms. Notice that the x-axis only\n",
    "goes as high as 1000. While we needed to look at large inputs to appreciate the\n",
    "difference between constant-time and logarithmic-time algorithms, the difference between logarithmic-time and linear-time algorithms is apparent even on\n",
    "small inputs. The dramatic difference in the relative performance of logarithmic\n",
    "and linear algorithms does not mean that linear algorithms are bad. In fact, most\n",
    "of the time a linear algorithm is acceptably efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](97growth.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot on the left in Figure 9.8 shows that there is a significant difference\n",
    "between O(n) and O(n\tlog(n)). Given how slowly log(n) grows, this may seem a\n",
    "bit surprising, but keep in mind that it is a multiplicative factor. Also keep in\n",
    "mind that in many practical situations, O(n\tlog(n)) is fast enough to be useful.\n",
    "On the other hand, as the plot on the right in Figure 9.8 suggests, there are many\n",
    "situations in which a quadratic rate of growth is prohibitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](98growth.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots in Figure 9.9 are about exponential complexity. In the plot on the\n",
    "left of Figure 9.9, the numbers to the left of the y-axis run from 0.0 to 1.2. However, the notation x1e301 on the top left means that each tick on the y-axis should\n",
    "be multiplied by 10301. So, the plotted y-values range from 0 to roughly 1.1* 10301.But it looks almost as if there are no curves in the plot on the left in Figure 9.9.\n",
    "That’s because an exponential function grows so quickly that relative to the y\n",
    "value of the highest point (which determines the scale of the y-axis), the y values\n",
    "of earlier points on the exponential curve (and all points on the quadratic curve)\n",
    "are almost indistinguishable from 0.\n",
    "The plot on the right in Figure 9.9 addresses this issue by using a logarithmic\n",
    "scale on the y-axis. One can readily see that exponential algorithms are impractical for all but the smallest of inputs.\n",
    "Notice that when plotted on a logarithmic scale, an exponential curve appears as a straight line. We will have more to say about this in later chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](99growth.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
